{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished3\n"
     ]
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "from tabulate import tabulate\n",
    "print 'finished3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (2034L,)\n",
      "test label shape: (677L,)\n",
      "dev label shape: (676L,)\n",
      "labels names: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "num_test = len(newsgroups_test.target)\n",
    "test_data, test_labels = newsgroups_test.data[num_test/2:], newsgroups_test.target[num_test/2:]\n",
    "mini_test_data, mini_test_labels = newsgroups_test.data[:5], newsgroups_test.target[:5]\n",
    "\n",
    "dev_data, dev_labels = newsgroups_test.data[:num_test/2], newsgroups_test.target[:num_test/2]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print 'training label shape:', train_labels.shape\n",
    "print 'test label shape:', test_labels.shape\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "print 'labels names:', newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1********************\n",
    "for i in range(0, 5): print \"LABEL IS:\",train_labels[i],\"\\nTEXT IS:\\n\", train_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is  26879\n",
      "Average number of non zero tokens across all data entries == 96.7059980334\n",
      "fraction of matrix/dictionary that is non-zero is very small ==  0.00359782722696\n",
      "first word is:  00\n",
      "last word is:  zyxel\n",
      "(2034, 4)\n",
      "Average number of non zero tokens across my dictionary == 0.268436578171\n",
      "Fraction of my matrix/dictionary that is non-zero == 24.1764995084\n"
     ]
    }
   ],
   "source": [
    "#2A)*********************\n",
    "clf = CountVectorizer()\n",
    "x = clf.fit_transform(train_data)\n",
    "print \"Vocabulary size is \", x.shape[1]\n",
    "CVmean = np.mean(x.getnnz(axis=1))\n",
    "print \"Average number of non zero tokens across all data entries ==\", CVmean\n",
    "print \"fraction of matrix/dictionary that is non-zero is very small == \", CVmean/x.shape[1]\n",
    "words = clf.get_feature_names()\n",
    "\n",
    "#2B)*********************\n",
    "print 'first word is: ', words[0]\n",
    "print 'last word is: ', words[x.shape[1]-1]\n",
    "\n",
    "#2C)*********************\n",
    "mywords = [\"atheism\", \"religion\", \"graphics\", \"space\"]\n",
    "myClf = CountVectorizer()\n",
    "mytrans = myClf.fit_transform(mywords)\n",
    "mine = myClf.transform(train_data)\n",
    "print mine.shape\n",
    "myMean = np.mean(mine.getnnz(axis=1))\n",
    "print \"Average number of non zero tokens across my dictionary ==\", myMean\n",
    "print \"Fraction of my matrix/dictionary that is non-zero ==\", CVmean/mine.shape[1]\n",
    "\n",
    "#code to cross check the mean**************\n",
    "# eachrow=[]\n",
    "# for i in range(0, 2034):\n",
    "#     #print x[i].shape\n",
    "#     eachrow.append(x[i].getnnz())\n",
    "#     #print x[i].getnnz()\n",
    "# print \"mean is equal to: \", np.mean(eachrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NGram Range = 1:\n",
      "vocabulary size is  26879\n",
      "\n",
      "NGram Range = 2:\n",
      "vocabulary size is  221770\n",
      "\n",
      "NGram Range = 3:\n",
      "vocabulary size is  537462\n"
     ]
    }
   ],
   "source": [
    "#2D)***********************\n",
    "# Ngrams, even at 2, significantly increase the vocabulary\n",
    "print \"\\nNGram Range = 1:\"\n",
    "clf = CountVectorizer(ngram_range=(1, 1))\n",
    "x = clf.fit_transform(train_data)\n",
    "print 'vocabulary size is ', x.shape[1]\n",
    "\n",
    "print \"\\nNGram Range = 2:\"\n",
    "clf = CountVectorizer(ngram_range=(1, 2))\n",
    "x = clf.fit_transform(train_data)\n",
    "print 'vocabulary size is ', x.shape[1]\n",
    "\n",
    "print \"\\nNGram Range = 3:\"\n",
    "clf = CountVectorizer(ngram_range=(1, 3))\n",
    "x = clf.fit_transform(train_data)\n",
    "print 'vocabulary size is ', x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Char_WB = 1,2:\n",
      "vocabulary size is  3167\n",
      "\n",
      "Char_WB = 2,2:\n",
      "vocabulary size is  3090\n",
      "\n",
      "Char_WB = 3,3:\n",
      "vocabulary size is  25864\n",
      "\n",
      "Char_WB = 4,4:\n",
      "vocabulary size is  75940\n",
      "\n",
      "Char_WB = 5,5:\n",
      "vocabulary size is  118537\n"
     ]
    }
   ],
   "source": [
    "#2D)***********************\n",
    "# the char_wb analyzer, which creates ngrams from only within word boundaries \n",
    "# makes a small vocab for smaller ngrams (less perumatations that occur repeatedly \n",
    "# in english languae) and larger for larger ngrams (more permutations)\n",
    "print \"\\nChar_WB = 1,2:\"\n",
    "clf = CountVectorizer(analyzer='char_wb', ngram_range=(1, 2))\n",
    "x = clf.fit_transform(train_data)\n",
    "print 'vocabulary size is ', x.shape[1]\n",
    "\n",
    "print \"\\nChar_WB = 2,2:\"\n",
    "clf = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n",
    "x = clf.fit_transform(train_data)\n",
    "print 'vocabulary size is ', x.shape[1]\n",
    "\n",
    "print \"\\nChar_WB = 3,3:\"\n",
    "clf = CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))\n",
    "x = clf.fit_transform(train_data)\n",
    "print 'vocabulary size is ', x.shape[1]\n",
    "\n",
    "print \"\\nChar_WB = 4,4:\"\n",
    "clf = CountVectorizer(analyzer='char_wb', ngram_range=(4, 4))\n",
    "x = clf.fit_transform(train_data)\n",
    "print 'vocabulary size is ', x.shape[1]\n",
    "\n",
    "print \"\\nChar_WB = 5,5:\"\n",
    "clf = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))\n",
    "x = clf.fit_transform(train_data)\n",
    "print 'vocabulary size is ', x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pruning words not in at least 10 docs, train data has this many words:  3064\n"
     ]
    }
   ],
   "source": [
    "#2E)**Prune words that are not in at least 10 docs  ******************************\n",
    "clf = CountVectorizer(min_df=10)\n",
    "x = clf.fit_transform(train_data)\n",
    "print \"After pruning words not in at least 10 docs, train data has this many words: \", x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has this many words:  26879\n",
      "dev data has this many words:  16246\n",
      "train has this many more words:  10633\n"
     ]
    }
   ],
   "source": [
    "#2F)****Difference in vocab sizes, train vs. dev data ***************************\n",
    "clf = CountVectorizer()\n",
    "x = clf.fit_transform(train_data)\n",
    "print \"train data has this many words: \", x.shape[1]\n",
    "clf = CountVectorizer()\n",
    "y = clf.fit_transform(dev_data)\n",
    "print \"dev data has this many words: \", y.shape[1]\n",
    "print \"train has this many more words: \", x.shape[1]-y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nV7PpOHYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aeee7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max score= 0.478882866053 when k=  112\n"
     ]
    }
   ],
   "source": [
    "#3) KNN ********************************\n",
    "# KNN doesn't work well because most texts share most of the same words, and only \n",
    "# a few words differentiate one topic from another\n",
    "# This is similar to the confusion in the MNIST data where numbers 4, 9 and 2, 8\n",
    "# were confused ==> they don't have enough distinct features to clearly differentiate\n",
    "clf = CountVectorizer()\n",
    "x = clf.fit_transform(train_data)\n",
    "xdev = clf.transform(dev_data)\n",
    "scores =[]\n",
    "k = []\n",
    "maxscore =0\n",
    "for i in range(1, 200):\n",
    "    k.append(i)\n",
    "    clfKNN = KNeighborsClassifier(n_neighbors=i)\n",
    "    clfKNN.fit(x, train_labels)\n",
    "    preds = clfKNN.predict(xdev)\n",
    "    score = metrics.f1_score(dev_labels, preds, average='weighted')\n",
    "    #print score\n",
    "    scores.append(score)\n",
    "    if score > maxscore: \n",
    "        maxscore = score\n",
    "        index=i    \n",
    "plt.plot(k, scores)\n",
    "plt.show()\n",
    "print 'max score=', maxscore, 'when k= ', index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.30629, std: 0.00552, params: {'alpha': 0.0}\n",
      "mean: 0.82547, std: 0.00573, params: {'alpha': 0.0001}\n",
      "mean: 0.82596, std: 0.00400, params: {'alpha': 0.001}\n",
      "mean: 0.82842, std: 0.00651, params: {'alpha': 0.01}\n",
      "mean: 0.82203, std: 0.01419, params: {'alpha': 0.1}\n",
      "mean: 0.80875, std: 0.01414, params: {'alpha': 0.5}\n",
      "mean: 0.79597, std: 0.01909, params: {'alpha': 1.0}\n",
      "mean: 0.77384, std: 0.01863, params: {'alpha': 2.0}\n",
      "mean: 0.68437, std: 0.02005, params: {'alpha': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IBM_ADMIN\\Anaconda2\\lib\\site-packages\\sklearn\\naive_bayes.py:664: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc)\n"
     ]
    }
   ],
   "source": [
    "# 3) MulitnomialNB: grid search for best alpha*************\n",
    "clf = CountVectorizer()\n",
    "x = clf.fit_transform(train_data)\n",
    "xdev = clf.transform(dev_data)\n",
    "scores =[]\n",
    "k = []\n",
    "parameters = {'alpha': [0.000, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "clf = GridSearchCV(MultinomialNB(), parameters)\n",
    "clf.fit(x, train_labels)\n",
    "for i in range(0, len(clf.grid_scores_)):\n",
    "    print clf.grid_scores_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regr score:  0.699704142012\n",
      "mean: 0.73402, std: 0.00750, params: {'C': 0.01}\n",
      "mean: 0.76647, std: 0.00299, params: {'C': 0.1}\n",
      "mean: 0.77139, std: 0.00337, params: {'C': 0.5}\n",
      "mean: 0.76303, std: 0.00306, params: {'C': 1.0}\n",
      "mean: 0.76254, std: 0.00448, params: {'C': 2.0}\n",
      "mean: 0.75320, std: 0.00062, params: {'C': 10.0}\n",
      "mean: 0.74975, std: 0.00438, params: {'C': 100.0}\n"
     ]
    }
   ],
   "source": [
    "#3) Logistic regression, find optimal value of C\n",
    "# Use grid search\n",
    "# Logistic regression does not work as well as NB because of different objectives \n",
    "# NB assigns each feature distinct probability correlated with frequency\n",
    "# whereas LogRegr attempts to minimize errors by generating estimate\n",
    "# weights across all features. This minimizes the impact of words that are \n",
    "# distinct/relevant to a specific class of news topic\n",
    "clf = CountVectorizer()\n",
    "x = clf.fit_transform(train_data)\n",
    "xdev = clf.transform(dev_data)\n",
    "regr = LogisticRegression()\n",
    "regr.fit(x, train_labels)\n",
    "print 'logistic regr score: ', regr.score(xdev, dev_labels)\n",
    "\n",
    "parameters = {'C': [0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 100.0]}\n",
    "clf = GridSearchCV(LogisticRegression(), parameters)\n",
    "clf.fit(x, train_labels)\n",
    "for i in range(0, len(clf.grid_scores_)):\n",
    "    print clf.grid_scores_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regr score:  0.699704142012\n",
      "\n",
      "for C value ==  0.01 the 4 coef squared are:\n",
      "0.0112213844262\n",
      "0.195811353216\n",
      "0.341946678825\n",
      "0.0168496947597\n",
      "for C value ==  0.1 the 4 coef squared are:\n",
      "5.65807705863\n",
      "12.6594418991\n",
      "10.2887959959\n",
      "4.46947852096\n",
      "for C value ==  0.5 the 4 coef squared are:\n",
      "62.2325192372\n",
      "63.0207267625\n",
      "55.9891650214\n",
      "53.2229374477\n",
      "for C value ==  1.0 the 4 coef squared are:\n",
      "173.935308315\n",
      "114.673169697\n",
      "123.464703362\n",
      "132.155194122\n",
      "for C value ==  2.0 the 4 coef squared are:\n",
      "385.444488356\n",
      "207.153538146\n",
      "321.515410377\n",
      "313.927054611\n",
      "for C value ==  10.0 the 4 coef squared are:\n",
      "1047.17465506\n",
      "735.038792014\n",
      "889.653095507\n",
      "971.995446525\n",
      "for C value ==  100.0 the 4 coef squared are:\n",
      "3424.66902027\n",
      "3222.07987749\n",
      "3638.80550815\n",
      "3180.67780003\n"
     ]
    }
   ],
   "source": [
    "#3C  Logistic Regression compare C value to coef_ squared *********\n",
    "clf = CountVectorizer()\n",
    "x = clf.fit_transform(train_data)\n",
    "xdev = clf.transform(dev_data)\n",
    "regr = LogisticRegression()\n",
    "regr.fit(x, train_labels)\n",
    "print 'logistic regr score: ', regr.score(xdev, dev_labels)\n",
    "print \n",
    "\n",
    "# Obviously sum of squares increases with C.\n",
    "# But for the optimal C value (based on the grid search above, C= ~0.5)\n",
    "# the sum of the coefficients squared at that value of C, is closest to the average \n",
    "# number of non-zero tokens for the entire corpus\n",
    "\n",
    "clf = CountVectorizer()\n",
    "x = clf.fit_transform(train_data)\n",
    "xdev = clf.transform(dev_data)        \n",
    "Cvalues=[0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 100.0]\n",
    "scores = []\n",
    "for i in range(0, len(Cvalues)):    \n",
    "    regr = LogisticRegression(penalty='l1', tol = 0.01, C=Cvalues[i])\n",
    "    regr.fit(x, train_labels)\n",
    "    allcoef = map(lambda x: np.sum(np.square(regr.coef_[x])),[0,1,2,3])\n",
    "    scores.append(allcoef)\n",
    "\n",
    "for i in range(0, len(Cvalues)):  \n",
    "    print 'for C value == ', Cvalues[i], 'the 4 coef squared are:' \n",
    "    for j in range(0,4):\n",
    "        print scores[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for  alt.atheism :\n",
      "atheism\n",
      "religion\n",
      "bobby\n",
      "atheists\n",
      "deletion\n",
      "top words for  comp.graphics :\n",
      "computer\n",
      "3d\n",
      "file\n",
      "image\n",
      "graphics\n",
      "top words for  sci.space :\n",
      "spacecraft\n",
      "launch\n",
      "nasa\n",
      "orbit\n",
      "space\n",
      "top words for  talk.religion.misc :\n",
      "order\n",
      "fbi\n",
      "blood\n",
      "christian\n",
      "christians\n",
      "feature       alt.atheism    comp.graphics    sci.space    talk.religion.misc\n",
      "----------  -------------  ---------------  -----------  --------------------\n",
      "atheism              0.94            -0.41        -0.45                 -0.43\n",
      "religion             0.95            -0.62        -0.79                 -0.06\n",
      "bobby                0.99            -0.22        -0.34                 -0.46\n",
      "atheists             1.03            -0.1         -0.32                 -0.84\n",
      "deletion             1.12            -0.4         -0.42                 -0.4\n",
      "computer             0.14             0.98        -0.68                 -0.49\n",
      "3d                  -0.36             1.12        -0.7                  -0.38\n",
      "file                -0.33             1.27        -0.81                 -0.63\n",
      "image               -0.58             1.35        -0.83                 -0.47\n",
      "graphics            -0.76             1.94        -1.34                 -0.76\n",
      "spacecraft          -0.36            -0.39         0.92                 -0.38\n",
      "launch              -0.47            -0.47         0.94                 -0.33\n",
      "nasa                -0.57            -0.48         1.01                 -0.47\n",
      "orbit               -0.41            -0.67         1.23                 -0.63\n",
      "space               -1.26            -1.32         2.16                 -1.17\n",
      "order               -0.79            -0.08        -0.15                  0.9\n",
      "fbi                 -0.31            -0.27        -0.45                  0.91\n",
      "blood               -0.53            -0.11        -0.32                  1.05\n",
      "christian           -0.61            -0.42        -0.27                  1.12\n",
      "christians          -0.74            -0.41        -0.53                  1.15\n"
     ]
    }
   ],
   "source": [
    "#4 Top Words*******************************\n",
    "clf = CountVectorizer()\n",
    "x = clf.fit_transform(train_data)\n",
    "regr = LogisticRegression()\n",
    "regr.fit(x, train_labels)\n",
    "\n",
    "myFeat = clf.get_feature_names()\n",
    "h =[]\n",
    "for i in range(0,4): h.append(newsgroups_train.target_names[i])\n",
    "eachsubject = []\n",
    "topwords =[]\n",
    "for i in range(0, 4):\n",
    "    eachsubject.append(sorted(range(len(regr.coef_[i])), key=lambda j:regr.coef_[i][j])[-5:])\n",
    "    print 'top words for ', h[i], ':'\n",
    "    for k in range (0, 5):\n",
    "        print myFeat[eachsubject[i][k]]\n",
    "    #topwords.append(np.argpartition(regr.coef_[i], -5)[-5:]) also works\n",
    "    topwords.append(sorted(range(len(regr.coef_[i])), key=lambda j:regr.coef_[i][j])[-5:])\n",
    "\n",
    "matrixList=[]\n",
    "eachrow=[]\n",
    "for i in range(0, 4):\n",
    "    for j in range(0, 5):\n",
    "        eachrow.append(myFeat[topwords[i][j]])\n",
    "        for k in range(0, 4):\n",
    "            eachrow.append(round(regr.coef_[k][topwords[i][j]], 2))            \n",
    "matrixList.append(eachrow)\n",
    "myheader = ['feature', h[0],h[1],h[2],h[3]]\n",
    "a=np.array(matrixList).reshape(20,5)\n",
    "print tabulate(a, headers=myheader, tablefmt=\"simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for  alt.atheism :\n",
      "you are\n",
      "look up\n",
      "cheers kent\n",
      "was just\n",
      "claim that\n",
      "top words for  comp.graphics :\n",
      "out there\n",
      "is there\n",
      "in advance\n",
      "comp graphics\n",
      "looking for\n",
      "top words for  sci.space :\n",
      "sherzer methodology\n",
      "and such\n",
      "sci space\n",
      "the moon\n",
      "the space\n",
      "top words for  talk.religion.misc :\n",
      "is strength\n",
      "the fbi\n",
      "compuserve com\n",
      "such lunacy\n",
      "ignorance is\n",
      "feature                alt.atheism    comp.graphics    sci.space    talk.religion.misc\n",
      "-------------------  -------------  ---------------  -----------  --------------------\n",
      "you are                       0.57            -0.32        -0.58                  0\n",
      "look up                       0.63            -0.24        -0.2                  -0.17\n",
      "cheers kent                   0.65            -0.88        -0.82                  0.6\n",
      "was just                      0.68            -0.19        -0.2                  -0.3\n",
      "claim that                    0.77            -0.26        -0.35                 -0.2\n",
      "out there                    -0.32             0.9         -0.58                 -0.33\n",
      "is there                     -0.43             0.91        -0.57                 -0.34\n",
      "in advance                   -0.54             0.97        -0.53                 -0.51\n",
      "comp graphics                -0.38             1.04        -0.47                 -0.4\n",
      "looking for                  -0.76             1.32        -0.61                 -0.7\n",
      "sherzer methodology          -0.16            -0.23         0.68                 -0.16\n",
      "and such                     -0.24            -0.4          0.69                 -0.26\n",
      "sci space                    -0.32            -0.39         0.74                 -0.27\n",
      "the moon                     -0.4             -0.58         0.95                 -0.24\n",
      "the space                    -0.31            -0.65         1.03                 -0.32\n",
      "is strength                  -0.18            -0.22        -0.2                   0.65\n",
      "the fbi                      -0.17            -0.26        -0.36                  0.65\n",
      "compuserve com               -0.13            -0.21        -0.2                   0.7\n",
      "such lunacy                  -0.15            -0.22        -0.19                  0.71\n",
      "ignorance is                 -0.23            -0.25        -0.21                  0.75\n"
     ]
    }
   ],
   "source": [
    "#4 Top Words Bigrams\n",
    "# The bigrams are surprising because they don't involve the most common/top words at all\n",
    "# which we might intuitively expect\n",
    "# Most of the top bi-grams are everyday phrases..'you are', 'was just', \n",
    "# 'out there', 'is there', 'and such', etc\n",
    "clf = CountVectorizer(ngram_range=(2, 2))\n",
    "x = clf.fit_transform(train_data)\n",
    "regr = LogisticRegression()\n",
    "regr.fit(x, train_labels)\n",
    "\n",
    "myFeat = clf.get_feature_names()\n",
    "h =[]\n",
    "for i in range(0,4): h.append(newsgroups_train.target_names[i])\n",
    "eachsubject = []\n",
    "topwords =[]\n",
    "for i in range(0, 4):\n",
    "    eachsubject.append(sorted(range(len(regr.coef_[i])), key=lambda j:regr.coef_[i][j])[-5:])\n",
    "    print 'top words for ', h[i], ':'\n",
    "    for j in range (0, 5):\n",
    "        print myFeat[eachsubject[i][j]]\n",
    "    #topwords.append(np.argpartition(regr.coef_[i], -5)[-5:])\n",
    "    topwords.append(sorted(range(len(regr.coef_[i])), key=lambda j:regr.coef_[i][j])[-5:])\n",
    "\n",
    "\n",
    "matrixList=[]\n",
    "eachrow=[]\n",
    "for i in range(0, 4):\n",
    "    for j in range(0, 5):\n",
    "        eachrow.append(myFeat[topwords[i][j]])\n",
    "        for k in range(0, 4):\n",
    "            eachrow.append(round(regr.coef_[k][topwords[i][j]], 2))            \n",
    "matrixList.append(eachrow)\n",
    "myheader = ['feature', h[0],h[1],h[2],h[3]]\n",
    "a=np.array(matrixList).reshape(20,5)\n",
    "print tabulate(a, headers=myheader, tablefmt=\"simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def emptyprocessor(s):\n",
    "    return s\n",
    "\n",
    "def mypreproc(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub('\\b\\w{7,50}\\b', ' i ', s)\n",
    "    s = re.sub('[\\w\\.-]+@[\\w\\.-]+', ' i ',s)\n",
    "    s= re.sub('\\_\\w+', ' i ', s)\n",
    "    s= re.sub('\\w+\\_', ' i ', s)\n",
    "    s = re.sub('[\\w+]+\\_+[\\w+]+', ' i ', s)\n",
    "    s = re.sub('[\\_+]+\\w+[\\_+]+', ' i ', s)    \n",
    "    s = re.sub('[\\w-]+\\d+[\\w-]+', ' i ', s)    \n",
    "    s = re.sub('[\\d-]+\\w+[\\d-]+', ' i ', s)    \n",
    "    s = re.sub('[\\d]+\\w+', ' i ', s)  \n",
    "    s = re.sub('[\\w+]+_+[\\w+]+', ' i ', s)  \n",
    "    s = re.sub('[\\_+]+\\w+[\\_+]+', ' i ', s) \n",
    "    s = re.sub('[_+]+\\w+[_+]+', ' i ', s) \n",
    "    s = re.sub('\\d+', ' i ', s)\n",
    "    s = re.sub('[.]+\\w+', ' i ', s)       \n",
    "    return s\n",
    "\n",
    "def mypreproc2(s):\n",
    "    s = s.lower()\n",
    "#   s = re.sub('\\b\\w{7,50}\\b', ' i ', s) Do not remove words larger than 7\n",
    "    s = re.sub('[\\w\\.-]+@[\\w\\.-]+', ' i ',s)\n",
    "    s= re.sub('\\_\\w+', ' i ', s)\n",
    "    s= re.sub('\\w+\\_', ' i ', s)\n",
    "    s = re.sub('[\\w+]+\\_+[\\w+]+', ' i ', s)\n",
    "    s = re.sub('[\\_+]+\\w+[\\_+]+', ' i ', s)    \n",
    "    s = re.sub('[\\w-]+\\d+[\\w-]+', ' i ', s)    \n",
    "    s = re.sub('[\\d-]+\\w+[\\d-]+', ' i ', s)    \n",
    "    s = re.sub('[\\d]+\\w+', ' i ', s)  \n",
    "    s = re.sub('[\\w+]+_+[\\w+]+', ' i ', s)  \n",
    "    s = re.sub('[\\_+]+\\w+[\\_+]+', ' i ', s) \n",
    "    s = re.sub('[_+]+\\w+[_+]+', ' i ', s) \n",
    "    s = re.sub('\\d+', ' i ', s)\n",
    "    s = re.sub('[.]+\\w+', ' i ', s)       \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default :\n",
      "vocabulary size = 26879\n",
      "avg f1 score: 0.694417287185\n",
      "avg score: 0.699704142012\n",
      "\n",
      "empty :\n",
      "vocabulary size = 33291\n",
      "avg f1 score: 0.69837564797\n",
      "avg score: 0.703402366864\n",
      "\n",
      "preprocessor #1 :\n",
      "vocabulary size = 22706\n",
      "avg f1 score: 0.703642237957\n",
      "avg score: 0.709072978304\n",
      "\n",
      "preprocessor #2 :\n",
      "vocabulary size = 22706\n",
      "avg f1 score: 0.706275532951\n",
      "avg score: 0.711908284024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5) Preprocessor  ************************** \n",
    "# Empty preprocessor makes the vocabulary larger (without significantly impacting scores)\n",
    "# because it overrides default string processing stage, which has built-in features for punctuation \n",
    "# other spurious characters\n",
    "# Preprocessor removes:\n",
    "#  Numbers, numbers tied to strings, and vice versa\n",
    "#  Underscore and undercores tied to strings and vice versa\n",
    "#  email addresses\n",
    "# Could only improve the score 1 basis point even when removing words over 7 chars\n",
    "\n",
    "avg_f1score = []\n",
    "avg_score = []\n",
    "names = ['default', 'empty', 'preprocessor #1', 'preprocessor #2']\n",
    "vectorizers =[]\n",
    "vectorizers.append(CountVectorizer())\n",
    "vectorizers.append(CountVectorizer(preprocessor=emptyprocessor))\n",
    "vectorizers.append(CountVectorizer(preprocessor=mypreproc, stop_words='english')) \n",
    "vectorizers.append(CountVectorizer(preprocessor=mypreproc2, stop_words='english'))\n",
    "\n",
    "for i in range(0, 4):\n",
    "    for j in range(0, 10):            \n",
    "        clf = vectorizers[i]\n",
    "        x = clf.fit_transform(train_data)\n",
    "        xdev = clf.transform(dev_data)\n",
    "        regr = LogisticRegression()\n",
    "        regr.fit(x, train_labels)\n",
    "        preds = regr.predict(xdev)\n",
    "        avg_f1score.append(metrics.f1_score(dev_labels, preds, average='weighted'))\n",
    "        avg_score.append(regr.score(xdev, dev_labels))\n",
    "    print names[i], ':'\n",
    "    print 'vocabulary size =', x.shape[1]\n",
    "    print 'avg f1 score:', sum(avg_f1score)/len(avg_f1score)\n",
    "    print 'avg score:',sum(avg_score)/len(avg_score)\n",
    "    print\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regr score l1:  0.686390532544\n",
      "for class alt.atheism non zero weights = 436\n",
      "for class comp.graphics non zero weights = 461\n",
      "for class sci.space non zero weights = 391\n",
      "for class talk.religion.misc non zero weights = 376\n",
      "\n",
      "\n",
      "logistic regr score l2: 0.699704142012\n",
      "for class alt.atheism non zero weights = 26879\n",
      "for class comp.graphics non zero weights = 26879\n",
      "for class sci.space non zero weights = 26879\n",
      "for class talk.religion.misc non zero weights = 26879\n"
     ]
    }
   ],
   "source": [
    "#6) *****************Logistic regression with L1************ \n",
    "# ***************** vs L2 ************ \n",
    "# Number of non-zero values for l1 regularization is much smaller than the \n",
    "# non zero number for l2,\n",
    "# because in l2 regularization all values are non-zero (but relatively small)\n",
    "clf = CountVectorizer()\n",
    "x = clf.fit_transform(train_data)\n",
    "xdev = clf.transform(dev_data)\n",
    "regr = LogisticRegression(penalty='l1')\n",
    "regr.fit(x, train_labels)\n",
    "print 'logistic regr score l1: ', regr.score(xdev, dev_labels)\n",
    "for i in range(0, 4):\n",
    "    print 'for class', newsgroups_train.target_names[i],'non zero weights =', np.count_nonzero(regr.coef_[i])    \n",
    "print\n",
    "print\n",
    "clf = CountVectorizer()\n",
    "x = clf.fit_transform(train_data)\n",
    "xdev = clf.transform(dev_data)\n",
    "regr = LogisticRegression(penalty='l2')\n",
    "regr.fit(x, train_labels)\n",
    "print 'logistic regr score l2:', regr.score(xdev, dev_labels)\n",
    "for i in range(0, 4):\n",
    "    print 'for class', newsgroups_train.target_names[i],'non zero weights =', np.count_nonzero(regr.coef_[i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c-value is:  0.01\n",
      "new vocab size is:  20\n",
      "Retrained l2 logistic regr score is:  0.488165680473\n",
      "c-value is:  0.1\n",
      "new vocab size is:  215\n",
      "Retrained l2 logistic regr score is:  0.693786982249\n",
      "c-value is:  0.25\n",
      "new vocab size is:  441\n",
      "Retrained l2 logistic regr score is:  0.681952662722\n",
      "c-value is:  0.5\n",
      "new vocab size is:  729\n",
      "Retrained l2 logistic regr score is:  0.677514792899\n",
      "c-value is:  0.75\n",
      "new vocab size is:  945\n",
      "Retrained l2 logistic regr score is:  0.671597633136\n",
      "c-value is:  10\n",
      "new vocab size is:  2446\n",
      "Retrained l2 logistic regr score is:  0.704142011834\n",
      "c-value is:  25\n",
      "new vocab size is:  4603\n",
      "Retrained l2 logistic regr score is:  0.695266272189\n",
      "c-value is:  50\n",
      "new vocab size is:  5878\n",
      "Retrained l2 logistic regr score is:  0.69674556213\n",
      "c-value is:  75\n",
      "new vocab size is:  6942\n",
      "Retrained l2 logistic regr score is:  0.701183431953\n",
      "c-value is:  100\n",
      "new vocab size is:  7741\n",
      "Retrained l2 logistic regr score is:  0.699704142012\n",
      "c-value is:  250\n",
      "new vocab size is:  13564\n",
      "Retrained l2 logistic regr score is:  0.701183431953\n",
      "c-value is:  500\n",
      "new vocab size is:  19169\n",
      "Retrained l2 logistic regr score is:  0.701183431953\n",
      "c-value is:  750\n",
      "new vocab size is:  21715\n",
      "Retrained l2 logistic regr score is:  0.699704142012\n",
      "c-value is:  1000\n",
      "new vocab size is:  23623\n",
      "Retrained l2 logistic regr score is:  0.701183431953\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAECCcAAAJAgHAEDi/wFJGzTATpTS5gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f5b1748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "#6) *******Prune L1 Logit with different values of C, create a new dictionary\n",
    "# Use new dictionary to score Logit using L2\n",
    "# After running the code below about 5 times, C = ~10 yields the max score \n",
    "# for most efficient(minimum)vocabulary, about 2500 words\n",
    "np.random.seed(0)    \n",
    "listofscores =[]\n",
    "vocabsizes =[]\n",
    "cvalues = [0.01,0.1,0.25,0.5,0.75,10,25,50,75,100,250,500,750,1000]\n",
    "mydict = {}\n",
    "clf = CountVectorizer()\n",
    "x = clf.fit_transform(train_data)\n",
    "vocab = clf.vocabulary_\n",
    "words=clf.get_feature_names()\n",
    "for i in range(0, len(cvalues)):\n",
    "    regr = LogisticRegression(penalty='l1', tol=0.01, C=cvalues[i])\n",
    "    regr.fit(x, train_labels) \n",
    "    uniquenp = np.unique(np.nonzero(map(lambda x: (regr.coef_[x]),[0,1,2,3])))\n",
    "    print 'c-value is: ', cvalues[i]\n",
    "    print 'new vocab size is: ', uniquenp.shape[0]\n",
    "    vocabsizes.append(uniquenp.shape[0])\n",
    "    mydict = {}\n",
    "    for j in range(0, uniquenp.shape[0]): \n",
    "        mydict.update({words[uniquenp[j]]:j})     \n",
    "    clf2 = CountVectorizer(vocabulary=mydict)\n",
    "    x2 = clf2.fit_transform(train_data)\n",
    "    xdev = clf2.transform(dev_data)\n",
    "    regr2 = LogisticRegression(penalty='l2')#default is l2\n",
    "    regr2.fit(x2, train_labels)\n",
    "    score = regr2.score(xdev, dev_labels)\n",
    "    listofscores.append(score)\n",
    "    print 'Retrained l2 logistic regr score is: ', score\n",
    "\n",
    "plt.scatter(vocabsizes, listofscores)\n",
    "plt.show()            \n",
    "print 'finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max prob is: 0.999978715534\n",
      "0.999978715534\n",
      "the counter 0\n",
      "\n",
      "\n",
      "Predicted label: comp.graphics\n",
      "\n",
      "\n",
      "Correct label: talk.religion.misc\n",
      "\n",
      "\n",
      "Index: 215\n",
      "\n",
      "Magnitude of folly 929.344142051\n",
      "I am pleased to announce that a *revised version* of _The Easy-to-Read Book\n",
      "of Mormon_ (former title: _Mormon's Book_) by Lynn Matthews Anderson is now\n",
      "available through anonymous ftp (see information below). In addition to the\n",
      "change in title, the revised ETR BOM has been shortened by several pages\n",
      "(eliminating many extraneous \"that's\" and \"of's\"), and many (minor) errors\n",
      "have been corrected. This release includes a simplified Joseph Smith Story,\n",
      "testimonies of the three and eight witnesses, and a \"Words-to-Know\"\n",
      "glossary.\n",
      "\n",
      "As with the previous announcement, readers are reminded that this is a\n",
      "not-for-profit endeavor. This is a copyrighted work, but people are welcome\n",
      "to make *verbatim* copies for personal use. People can recuperate the\n",
      "actual costs of printing (paper, copy center charges), but may not charge\n",
      "anything for their time in making copies, or in any way realize a profit\n",
      "from the use of this book. See the permissions notice in the book itself\n",
      "for the precise terms.\n",
      "\n",
      "Negotiations are currently underway with a Mormon publisher vis-a-vis the\n",
      "printing and distribution of bound books. (Sorry, I'm out of the wire-bound\n",
      "\"first editions.\") I will make another announcement about the availability\n",
      "of printed copies once everything has been worked out.\n",
      "\n",
      "FTP information: connect via anonymous ftp to carnot.itc.cmu.edu, then \"cd\n",
      "pub\" (you won't see anything at all until you do).\n",
      "\n",
      "\"The Easy-to-Read Book of Mormon\" is currently available in postscript and\n",
      "RTF (rich text format). (ASCII, LaTeX, and other versions can be made\n",
      "available; contact dba@andrew.cmu.edu for details.) You should be able to\n",
      "print the postscript file on any postscript printer (such as an Apple\n",
      "Laserwriter); let dba know if you have any difficulties. (The postscript in\n",
      "the last release had problems on some printers; this time it should work\n",
      "better.) RTF is a standard document interchange format that can be read in\n",
      "by a number of word processors, including Microsoft Word for both the\n",
      "Macintosh and Windows. If you don't have a postscript printer, you may be\n",
      "able to use the RTF file to print out a copy of the book.\n",
      "\n",
      "-r--r--r--  1 dba                   1984742 Apr 27 13:12 etrbom.ps\n",
      "-r--r--r--  1 dba                   1209071 Apr 27 13:13 etrbom.rtf\n",
      "\n",
      "For more information about how this project came about, please refer to my\n",
      "article in the current issue of _Sunstone_, entitled \"Delighting in\n",
      "Plainness: Issues Surrounding a Simple Modern English Book of Mormon.\"\n",
      "\n",
      "Send all inquiries and comments to:\n",
      "\n",
      "    Lynn Matthews Anderson\n",
      "    5806 Hampton Street\n",
      "    Pittsburgh, PA 15206\n",
      "\n",
      "\n",
      "Predicted label: comp.graphics\n",
      "\n",
      "\n",
      "Correct label: talk.religion.misc\n",
      "\n",
      "\n",
      "Index: 665\n",
      "\n",
      "Magnitude of folly 324.987922787\n",
      "Can anyone provide me a ftp site where I can obtain a online version\n",
      "of the Book of Mormon. Please email the internet address if possible.\n",
      "\n",
      "\n",
      "Predicted label: talk.religion.misc\n",
      "\n",
      "\n",
      "Correct label: alt.atheism\n",
      "\n",
      "\n",
      "Index: 607\n",
      "\n",
      "Magnitude of folly 287.179159461\n",
      "\n",
      "The 24 children were, of course, killed by a lone gunman in a second story\n",
      "window, who fired eight bullets in the space of two seconds...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7)***********************\n",
    "# TDIDFVectorizer weights words by how often they appear in a document and\n",
    "# weights that against the frequency of the word in the entire corpus thereby\n",
    "# offseting high weights for words common to the corpus\n",
    "\n",
    "# Create list of all correct predictions to sort to find \n",
    "# largest value of (Max prob) / (prob of the correct prediction) \n",
    "# i.e. we are technically looking for smallest probability for a correct prediction\n",
    "\n",
    "\n",
    "# Analyzing the top 3 docucuments that scored the highest for ratio of:\n",
    "#  Max_Prob/(Prob for a correct prection)\n",
    "# first 2 miscoded docs used \"FTP\" and \"Email\" and were classified as graphics instead of religion (correct label)\n",
    "#  One issue is that words used in atheism and religion are closely related in general as related topics\n",
    "#  Another issue with the 2nd & 3rd document is the short length (which can't be controlled for but poses a challenge)\n",
    "\n",
    "#  One way to improve the scoring is to use the L1 penalty, assigning absolute weights\n",
    "#  to more clearly distinguish words characteristic of a class\n",
    "\n",
    "# And if documents could be correlated to authors/email addresses\n",
    "# it would give better insight on what the topic might be\n",
    "\n",
    "# In running the reverse test, (commented out below) , the results show\n",
    "# top documents are very long document lengths, high word frequencies which \n",
    "# increases the probability of words characteristic of the given class\n",
    "\n",
    "maxscore=0\n",
    "otherscores=[]\n",
    "clf = TfidfVectorizer()\n",
    "x = clf.fit_transform(train_data)\n",
    "xdev = clf.transform(dev_data)\n",
    "regr = LogisticRegression(C=100.0)\n",
    "regr.fit(x, train_labels)\n",
    "preds = regr.predict(xdev)\n",
    "probs= regr.predict_proba(xdev)\n",
    "maxprob = 0\n",
    "mycounter = 0\n",
    "incorrect = []\n",
    "for prob in probs:\n",
    "    if max(prob) > maxprob:\n",
    "        maxprob = max(prob)\n",
    "print 'max prob is:', maxprob        \n",
    "print maxprob\n",
    "#Create list of all correct predictions to sort to find \n",
    "# largest value of (Max prob) / (prob of the correct prediction) \n",
    "# i.e. we are technically looking for smallest probability for a correct prediction\n",
    "# capture index of dev_data as counter, label, and proability of this prediction \n",
    "print 'the counter', mycounter\n",
    "for pred, prob, label in zip(preds, probs, dev_labels):\n",
    "    if pred != label:\n",
    "        incorrect.append([mycounter, pred, label, max(prob)/prob[label]])\n",
    "    mycounter+=1\n",
    "\n",
    "sortedlist = sorted(incorrect, key=lambda oneitem: oneitem[3], reverse=True)\n",
    "for i in range (0,3):\n",
    "    print '\\n\\nPredicted label:' ,newsgroups_train.target_names[sortedlist[i][1]]\n",
    "    print '\\n\\nCorrect label:' ,newsgroups_train.target_names[sortedlist[i][2]]\n",
    "    print '\\n\\nIndex:' ,sortedlist[i][0]    \n",
    "    print '\\nMagnitude of folly', sortedlist[i][3]\n",
    "    print dev_data[sortedlist[i][0]]\n",
    "        \n",
    "# Check reverse list \n",
    "# Results show very long document lengths, high word frequencies\n",
    "\n",
    "# sortedlist = sorted(correct, key=lambda oneitem: oneitem[2], reverse=True)\n",
    "# for i in range (0,3):\n",
    "#     print '\\n\\nCorrect label:' ,sortedlist[i][1]\n",
    "#     print '\\n\\nIndex:' ,sortedlist[i][0]    \n",
    "#     print '\\nProbability', sortedlist[i][2]\n",
    "#     print dev_data[sortedlist[i][0]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
